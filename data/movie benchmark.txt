201904new:
# TF：
    #xavier_init; lr=0.002; no L2.
    # FM avg=0.94162
    # LR avg=0.96164
    # DNNavg=0.9523
    # FieldDNN=0.9619

    # Wide:          EPOCH=150 LR=0.001 0.9626 0.9627
    # Deep(10,10)    EPOCH=150 LR=0.001 0.9534 0.9550 [0.9663x激活]
    # Wide&Deep:     EPOCH=150 LR=0.001 0.9527 0.9542 0.9588 0.9596 0.9612 ; EPOCH=150 LR=0.0005 0.9507 0.9521 0.9537 0.9533 0.9538 0.9607 0.9628
    # AVG 0.001LR=0.9573 / 0.0005LR=0.9553   #pd.Series([float(i) for i in "0.9507 0.9521 0.9537 0.9533 0.9538 0.9607 0.9628".split(" ")]).mean()

    #Deep(10,10) lr0.001 MEAN=0.9611 BEST=0.9541 0.95416194,0.9596375,0.96367896,0.9652838,0.96610063,0.9588475,0.95897835,0.96488595,0.9621352,0.9573141,
    #Deep+Wide   lr0.001 MEAN=0.9581 BEST=0.9516 0.95761794,0.9650,0.95778,0.95893,0.95407,0.96138,0.95165,0.95456,0.95796,0.9627469

3-> #Deep(10,10)  lr0.0005 MEAN=0.9526 BEST=0.9469  0.9560482,0.95132154,0.94783217,0.9644138,0.9591737,0.9489759,0.9468578,0.95053107,0.9541365,0.94731116
4-> #Deep+Wide    lr0.0005 MEAN=0.9564 BEST=0.9493 0.95601565,0.953820,0.9520745, 0.9509112,0.95234793,0.96418005,0.9615978,0.9618426,0.9493099,0.9622372,
2-> #Deep+FM      lr0.0005:MEAN=0.9509 BEST=0.9461  [0.9492949, 0.9602662, 0.95251215, 0.95207393, 0.94615424, 0.94697815, 0.9534201, 0.9474273, 0.95077604, 0.95029736]
1-> #Deep+Wide+FM lr0.0005:MEAN=0.9497 BEST=0.9450  [0.9480593, 0.9516951, 0.9508657, 0.9450281, 0.949079, 0.9505722, 0.95087147, 0.95070416, 0.9515439, 0.94895566]

3->#FieldDNN(e20,10,10):   MEAN=0.9557 BEST=0.9502 [0.9593348, 0.95224166, 0.95357233, 0.9642616, 0.95332587, 0.9522401, 0.9571624, 0.9508174, 0.96372133, 0.9501557]
4->#FieldDNN(Embd)+Wide:   MEAN=0.9589 BEST=0.9520 [0.95276886, 0.95402354, 0.95538324, 0.966199, 0.9519791, 0.9525499, 0.9685491, 0.9668559, 0.96609026, 0.9541986]
1->#FieldDNN(Embd)+FM:     MEAN=0.9477 BEST=0.9422 [0.9451019, 0.9461878, 0.9434496, 0.9437016, 0.9485915, 0.9457546, 0.94225216, 0.942883, 0.945795, 0.9728427]
2->#FieldDNN(Embd)+Wide+FM:MEAN=0.9497 BEST=0.9463 [0.9478442, 0.9563259, 0.94867826, 0.9612056, 0.94678885, 0.9484733, 0.9467022, 0.94633967, 0.94688517, 0.94801724]

改进Field_DNN和LR，只用ids输入：
    #LR: 1.0415 为什么不如正常的0.961？值得思考  修改bug后:LR_ 增加了reduce_sum at axis=1 Best Score: 1.1053765
    #FM               0.9613,0.9500   [0.9527672,0.95569927,0.9500226,0.9731106,0.9540914,0.96675104,0.9648452,0.9679305,0.96664655,0.9621126]
    #Deep(e20,10,10): 0.9579 0.9485   [0.9646315, 0.96457046, 0.9651331, 0.9561693, 0.9657518, 0.953225, 0.9517628, 0.9543784, 0.95514125, 0.94852614]
    #Deep+LR:    MEAN=0.9592 0.9501   [0.9612582, 0.95976895, 0.9532099, 0.9658031, 0.9501344, 0.95863473, 0.95315826, 0.96515125, 0.958458, 0.96695334]
    #Deep+LR_:   MEAN=0.9510 0.9408   [0.94129854, 0.95424414, 0.9582963, 0.9503197, 0.94088507, 0.956375, 0.9425513, 0.9542897, 0.9559774, 0.956527]
    #Deep+FM:    MEAN=0.9581 0.9519   [0.95319426, 0.9659179, 0.9583129, 0.9625754, 0.9568584, 0.9555263, 0.9519126, 0.95558035, 0.96644086, 0.95500976]
 #Deep+LR_+FM:   MEAN=0.9476 0.9400   [0.9490024, 0.94657576, 0.9401189, 0.94825876, 0.95636916, 0.95200694, 0.94006425, 0.94265074, 0.94319284, 0.9583263]
 -> #DeepFM超过原始FM，更超过embeddingLookUp的FM啦！

 FM2是等价的FM实现方式：每个field，和平方减平方和(None,k)，最后求和再*0.5
 Deep+LR_+FM2:   MEAN=0.9474 0.9413   [0.94403505, 0.944647, 0.95472664, 0.9430393, 0.9469636, 0.94908786, 0.95835567, 0.9435927, 0.94131535, 0.9480291]
(new computer CPU: LR1.1897~1.19 ;FM 1.02~1.03 GPU:LR1.187~1.19 FM:1.02)
-------------------

#movie lens 1M
    Feature: user_id+ movie_id
         LR: 1.1136   [1.11374,1.11363,1.11355]
   (K=16)FM: 0.8759 0.8725 [0.8725196239314502, 0.8778558374960211, 0.8761410918416856, 0.8767934833900838, 0.8764116537721851]
(2k.k.k)MLP: 0.8909 0.8874 [0.8905830239947838, 0.8876157716859745, 0.8874417751650266, 0.8929219301742843, 0.8958153680910038]
LRWide&Deep: 0.8894 0.8864 [0.8905002132246765, 0.8884505889083766, 0.8942822494084322, 0.8872838225545763, 0.886356520652771]
FMWide&Deep: 0.8876 0.8862 [0.8891306483292881, 0.8864404663254943, 0.8862015399751784, 0.8870562556423719, 0.8892361102224905]
     DeepFM: 0.8915 0.8882 [0.8891181369371052, 0.8885330240937728, 0.8965300496620467, 0.894879262809512, 0.8881970309003999]

--
同参数Feature: user_id+movie_id+gender+age+occ [6040, 3706, 2, 7, 21]
         LR:1.1138
         FM:0.8900 0.8874
        MLP:0.8869 0.8827 [0.8826913429211967, 0.8865704888029944, 0.8894205404233329, 0.8848255771624891, 0.8911240322680413]
FMWide&Deep:0.8826 0.8802 [0.8823739401901824, 0.8837272431277021, 0.8806559122061428, 0.8857936966268322, 0.880220939388758]
     DeepFM:0.8820 0.8801 [0.8826602427265312, 0.8804598394828507, 0.8817998533007465, 0.8800670077529135, 0.8852888055994541]

   ---->AFM:0.8766 0.8739 [0.8756965901278242, 0.8739465453956701, 0.8757859254185157, 0.878804661503321, 0.8788787903664987]  比普通FM好了15个千分点 比最好的DeepFM还高6个千
[new envirment test]
   (GPU)AFM:0.8780
(GPU B=2000)0.8857 0.8848 [0.8861,0.8837,0.8848,0.8876,0.8866] #比b=500差了10个千，是否尝试下BN?
        输出model.全1.
(new computer CPU AFM : [0.877 0.878 0.879 0.880 0.881]| GPU AFM:[0.877,0.876,0.879])
(new computer CPU FM : | GPU FM:MEAN=0.8895 BEST=0.8873 FM无论CPU/GPU就跟旧设备一样. [0.8929,0.8888,0.8895,0.8888,0.8874])
        NFM:0.8855 0.8833 [0.8867756211304967, 0.8886955860294873, 0.8853611725795119, 0.883353234544585, 0.8833889285220375] 比普通FM好了7个千分，但比AFM差8个千（新设备几乎没有影响）.
    DeepAFM:0.8813 0.8787 [0.881185443793671, 0.8795960338809822, 0.8787062863760357, 0.8843611107596868, 0.8827619403223448] 比AFM低了4个千.说明MLP和LR把AFM搞差了.

    #1.N-AFM? 2.attention vs matmul vs conv 3.dual attention / attention deep cross
    1.
    2.#dual attention 并行 同时对c和k做attention：0.892  先对c再对k:0.89234 0.8872  ; 对最后的k做：0.8972
    3. 全连接和attention的区别：全连接是每个维度固定权重的映射，而attention是每个维度
    【SimpleAFM】:based on matmul!
    (0.8799 0.8779 [0.8779658202883563, 0.879289800004114, 0.8794466397430324, 0.8788896631590928, 0.8842831839489032])比AFM略差一丝，基本一样但有一个模型偏差大.
    原理(None,c,k)  mat W':c,1 --> (None,k,1)  mat p:k,1 -->(None,1)

    【ConvAFM】:based on conv!超越AFM三个千分点! current Best.
     -> matmul style:
    0.8724 0.8712 [0.8727966151660002, 0.873124818862239, 0.8726405109031291, 0.8723721763755702, 0.8712409137170526]
     -> conv2d style: 确认一致.  inp=oup=1
    0.8725 0.8706 [0.8716835280007954, 0.8745225996910772, 0.8705774947057796, 0.8743310920799835, 0.8714246131196807]
     -> inp=1,oup=2.
    0.8756 0.8727 [0.875331884547125, 0.8797903753534148, 0.8757225093962271, 0.8727051543284066, 0.8745067042640493]
    -> inp=1,oup=3.
    0.8776 0.8752 [0.8757838303529764, 0.875153649758689, 0.8797498521925528, 0.8811651723294318, 0.8763137322437914]
    -> inp=1,oup=4
    0.8772 0.8744 [0.8773693012285836, 0.8743984021717989, 0.8810061142414432, 0.8776589965518516, 0.8754049851924558]
    【Conv NFM】:
    nn=(c,8,4,1) 四个输出：0.8832 0.8804 [0.886200704152071, 0.8825128842003738, 0.8803520619114743, 0.8827658917330489, 0.8841606036017212]
    nn=(c,4,1)   三个输出：0.8827 0.8807 [0.8829400008237814, 0.8808369579194467, 0.8830305031583279, 0.8859907278531715, 0.8807316274582585]
    nn=(c,4)     两个输出: 0.8788 0.8748 [0.8794441081300567, 0.8788113112691083, 0.8838653140430209, 0.8769311541243444, 0.8748137321653245]
    nn=(c,1)     两个输出: 0.8739 0.8707 [0.8748212328440026, 0.8721325057971326, 0.8706626587276217, 0.8745743220365501, 0.8773065289364586]


同特征 对FM减掉依赖
         FM:0.8843 0.8827 [0.8837845201733746, 0.8834048621262176, 0.8850899094267737, 0.8827350067186959, 0.8866759037669701]  比全交叉FM好5个千
FMWide&Deep:0.8821 0.8779 [0.8836640089373046, 0.8844558421569535, 0.8779307045514071, 0.8817171700393097, 0.8827627062797546]  比全交叉FWD好2个千
     DeepFM:0.8798 0.8792 [0.8800812885731082, 0.8791612175446523, 0.879225922838042, 0.8800252790692487, 0.8807229491728771]   比全交叉DFM好3个千
        AFM:0.8825 0.8805 [0.883282467081577, 0.8826788224751436, 0.8825482526911965, 0.880550700048857, 0.8835271959063373]    比全交叉AFM【低】了7个千.说明attention已经capture到了


--
同参数Feature: user_id+movie_id+(gender+age+occ)+(genre) [6040, 3706, 2, 7, 21] 不收敛..? LR~1.117@EPOCH=2/3/4  MLP~1.118@EPOCH=1
--
do_1:+ movie genre 爆炸
do_2:+ FM消除依赖 FM_DependencyEliminate
do_3:+ CIN
--------------------------------

# sklearn:
    Ridge 0.961;
    Lasso 1.122;
    ElasticNet:1.122

# lgb:
    1.00835 (best param: 'max_depth':30,'num_leaves': 200)


下面tf不准：
=====================================
sklearn:
    lr:系数爆炸
    ridge:0.961
    lasso/elasticNet:1.122
lgb:
    1.00835 (best param: 'max_depth':30,'num_leaves': 200 )
tf:
    lr(w l2_norm):1.007 (best param:batch size=500,adam 0.01)
    fm:0.95373  (best param:batch size=500,adam 0.01)



-------------------------
movielens latest. tag prediction rmse lr=0.0005 batch=1024 early_stopping_rounds=15
        LR: 0.9458
        FM:Best Score: 0.7367297162303766  at round  14   0.7423419489623597  at round  15
        MLP:Best Score: 0.7887067210476838  at round  11
        DFM:Best Score: 0.7972475506874345  at round  10
        NFM:Best Score: 0.7448198249251206  at round  3
        AFM:Best Score: 0.7693624980207803  at round 13
       CAFM:Best Score: 0.7270582996416783  at round  8
       CNFM:Best Score: 0.7277697468519705  at round  4

re_run: lr=0.0002,N_EPOCH=100,batch_size=1024,early_stopping_rounds=15
    FM   0.7418 0.7374 [0.7420544477476589, 0.7432679714129825, 0.7463349678378174, 0.7373948548274504, 0.7397591127371936]
    MLP  0.8172 0.8005 [0.8005491305705676, 0.8304985414874233, 0.806352101312661, 0.8252118271206723, 0.8233442148560076]
    DFM  0.7985 0.7797 [0.8071864117129742, 0.8064103320888851, 0.8047545949618021, 0.7796648124360149, 0.7946412784219035]
    NFM  0.7474 0.7359 [0.7358814849986793, 0.7629101249865616, 0.7600697449892451, 0.7414941661846564, 0.7366691137322728]
    AFM  0.7586 0.7448 [0.754342629364065, 0.7447536173074142, 0.7647130870671006, 0.7620551974877067, 0.7670084105391927]
    CAFM 0.7274 0.7141 [0.718197048327444, 0.7141293010232859, 0.7403262024340422, 0.7151694378620843, 0.7489307504262983]
    CNFM 0.7250 0.7240 [0.7251382016617319, 0.7243603084151542, 0.7269466616228747, 0.7240004010082032, 0.7245057860763423]

----------------------------
终于发现BUG！！解决LR中的没有keepdims的问题
movie lens AFM: rounded_rmse K=256,lr=0.0002,N_EPOCH=100,batch_size=1024,early_stopping_rounds=15
     LR：0.5925
     FM：0.4707 at round  100
    MLP: 0.5526@20 0.5543@18 0.5545@26

    lr=0.0004,N_EPOCH=100,batch_size=1024,early_stopping_rounds=15:
    FM:0.47583942247498817  at round  99

    lr=0.001,N_EPOCH=100,batch_size=4096,early_stopping_rounds=15:
              FM 0.4716 0.4616 [0.4615745090797523, 0.47751223197442794, 0.48188190272961845, 0.4654127593302519, 0.47144765939299155]
             MLP 0.5547 0.5498 [0.5521643645579941, 0.5497829604146859, 0.5619770722332953, 0.5559272731911425, 0.5535220250555853]
    (layer=1)NFM 0.5026 0.4883 [0.49492170541934416, 0.4983836790237002, 0.4882567305561861, 0.5114914719708186, 0.5198597840151992]
    (layer=2)NFM 0.4799 0.4741 [0.4808606069718846, 0.4754995542844974, 0.4740632420448006, 0.4890896348872503, 0.47978667740726133]
(l2继续尝试BN&DROP调优)
(afm_factor256)
            AFM 0.4641 0.4579 [0.45789503709436424, 0.45887810412995106, 0.4742138503255528, 0.46002649873254, 0.4695296085442708]
            CFM 0.4449 0.4386 [0.44946042875928, 0.458209527794555, 0.4391687542211156, 0.43859848788728134, 0.4389438245490222]
  CFM 10 TIMES: 0.4445 0.4386 [0.44946042875928, 0.458209527794555, 0.4391687542211156, 0.43859848788728134, 0.4389438245490222, 0.45537909361138035, 0.43946298414329715, 0.44139103748948294, 0.4438456694458008, 0.4409773219274019]


            AFM(factor=8加速训练)：
                AFM(features_sizes, k=256, attention_FM=8,dropout_keeprate=0.9,lambda_l2=0.0)
                0.4568 0.4547 [0.4572698326594009, 0.4573921705527199, 0.45465291076328773, 0.45600533839921154, 0.45853017602842766] @about 70

 lambda_l2=0.01 0.4600 0.4533 [0.4533383172789684, 0.4556978576284572, 0.4559235026324993, 0.46789378198860504, 0.4673594533672846]
  lambda_l2=0.1 0.4545 0.4517 [0.4539906716335104, 0.45171106174205944, 0.45947772612652515, 0.45273780400588653]
  lambda_l2=0.5 0.4569 0.4505 [0.45365546628652675, 0.45050660377710666, 0.466321902291068, 0.45288184553374505, 0.46128432840955386] ==> AFM BEST
  lambda_l2=1.0 0.4628 0.4558 [0.46781114589352274, 0.46781114589352274, 0.45993904670218094, 0.455809302852129]
                AFM(features_sizes, k=256, attention_FM=8,dropout_keeprate=0.9,lambda_l2=0.5)
                0.4600 0.4533 [0.4533383172789684, 0.4556978576284572, 0.4559235026324993, 0.46789378198860504, 0.4673594533672846]

                keetrate=0.8: drop@FM out:lr=0.001 0.455*2 0.458 0.459 @about 90 增大学习率
                keetrate=0.8: drop@FM out:lr=0.002 0.458*2 0.461
                keetrate=0.5: drop@FM out:lr=0.002 0.470 0.475~0.576@50

            CFM drop=0.9 lambda_l2=0.0
            0.4399 0.4377 [0.4401421597430921, 0.4445318712155105, 0.4386408222350688, 0.43773401371670007, 0.4382561142294935] ==> CFM BEST
            CFM drop=0.8 lambda_l2=0.0
            0.4428 0.4416 [0.4453344480139384, 0.44203669836782483, 0.4432974347651861, 0.4416111090133734, 0.4416171192475049]
            CFM drop=0.9 lambda_l2=0.001
            0.4460 0.4445 [0.4445430558011853, 0.4463642851494561, 0.44487508261756536, 0.446464587102342, 0.44768517511340944]
            CFM drop=0.9 lambda_l2=0.01
            0.4599 0.4575 [0.45806877558385833, 0.45753119090607136, 0.46346873364072205, 0.4595067564539701, 0.46098559930914673]
            CFM drop=0.9 lambda_l2=0.1
            炸了 [0.4894827117259846,0.5290414669703993,0.546778893353624,0.5028531207900812]

            drop错了不能drop embed：
            AFM(factor=8)  +drop0.5: 0.49338105986929937  at round  90 (似乎还可以继续收敛)
            AFM(factor=256)+drop0.5: 0.4966860595486163   at round  89
考虑dropout要加大学习率     +drop0.5: lr*10=0.01 0.5225 at 25 错

1.drop for conv2d
2.+reg +drop
3.引用AFM的论文


