201904new:
# TF：
    #xavier_init; lr=0.002; no L2.
    # FM avg=0.94162
    # LR avg=0.96164
    # DNNavg=0.9523
    # FieldDNN=0.9619

    # Wide:          EPOCH=150 LR=0.001 0.9626 0.9627
    # Deep(10,10)    EPOCH=150 LR=0.001 0.9534 0.9550 [0.9663x激活]
    # Wide&Deep:     EPOCH=150 LR=0.001 0.9527 0.9542 0.9588 0.9596 0.9612 ; EPOCH=150 LR=0.0005 0.9507 0.9521 0.9537 0.9533 0.9538 0.9607 0.9628
    # AVG 0.001LR=0.9573 / 0.0005LR=0.9553   #pd.Series([float(i) for i in "0.9507 0.9521 0.9537 0.9533 0.9538 0.9607 0.9628".split(" ")]).mean()

    #Deep(10,10) lr0.001 MEAN=0.9611 BEST=0.9541 0.95416194,0.9596375,0.96367896,0.9652838,0.96610063,0.9588475,0.95897835,0.96488595,0.9621352,0.9573141,
    #Deep+Wide   lr0.001 MEAN=0.9581 BEST=0.9516 0.95761794,0.9650,0.95778,0.95893,0.95407,0.96138,0.95165,0.95456,0.95796,0.9627469

3-> #Deep(10,10)  lr0.0005 MEAN=0.9526 BEST=0.9469  0.9560482,0.95132154,0.94783217,0.9644138,0.9591737,0.9489759,0.9468578,0.95053107,0.9541365,0.94731116
4-> #Deep+Wide    lr0.0005 MEAN=0.9564 BEST=0.9493 0.95601565,0.953820,0.9520745, 0.9509112,0.95234793,0.96418005,0.9615978,0.9618426,0.9493099,0.9622372,
2-> #Deep+FM      lr0.0005:MEAN=0.9509 BEST=0.9461  [0.9492949, 0.9602662, 0.95251215, 0.95207393, 0.94615424, 0.94697815, 0.9534201, 0.9474273, 0.95077604, 0.95029736]
1-> #Deep+Wide+FM lr0.0005:MEAN=0.9497 BEST=0.9450  [0.9480593, 0.9516951, 0.9508657, 0.9450281, 0.949079, 0.9505722, 0.95087147, 0.95070416, 0.9515439, 0.94895566]

3->#FieldDNN(e20,10,10):   MEAN=0.9557 BEST=0.9502 [0.9593348, 0.95224166, 0.95357233, 0.9642616, 0.95332587, 0.9522401, 0.9571624, 0.9508174, 0.96372133, 0.9501557]
4->#FieldDNN(Embd)+Wide:   MEAN=0.9589 BEST=0.9520 [0.95276886, 0.95402354, 0.95538324, 0.966199, 0.9519791, 0.9525499, 0.9685491, 0.9668559, 0.96609026, 0.9541986]
1->#FieldDNN(Embd)+FM:     MEAN=0.9477 BEST=0.9422 [0.9451019, 0.9461878, 0.9434496, 0.9437016, 0.9485915, 0.9457546, 0.94225216, 0.942883, 0.945795, 0.9728427]
2->#FieldDNN(Embd)+Wide+FM:MEAN=0.9497 BEST=0.9463 [0.9478442, 0.9563259, 0.94867826, 0.9612056, 0.94678885, 0.9484733, 0.9467022, 0.94633967, 0.94688517, 0.94801724]

改进Field_DNN和LR，只用ids输入：
    #LR: 1.0415 为什么不如正常的0.961？值得思考  修改bug后:LR_ 增加了reduce_sum at axis=1 Best Score: 1.1053765
    #FM               0.9613,0.9500   [0.9527672,0.95569927,0.9500226,0.9731106,0.9540914,0.96675104,0.9648452,0.9679305,0.96664655,0.9621126]
    #Deep(e20,10,10): 0.9579 0.9485   [0.9646315, 0.96457046, 0.9651331, 0.9561693, 0.9657518, 0.953225, 0.9517628, 0.9543784, 0.95514125, 0.94852614]
    #Deep+LR:    MEAN=0.9592 0.9501   [0.9612582, 0.95976895, 0.9532099, 0.9658031, 0.9501344, 0.95863473, 0.95315826, 0.96515125, 0.958458, 0.96695334]
    #Deep+LR_:   MEAN=0.9510 0.9408   [0.94129854, 0.95424414, 0.9582963, 0.9503197, 0.94088507, 0.956375, 0.9425513, 0.9542897, 0.9559774, 0.956527]
    #Deep+FM:    MEAN=0.9581 0.9519   [0.95319426, 0.9659179, 0.9583129, 0.9625754, 0.9568584, 0.9555263, 0.9519126, 0.95558035, 0.96644086, 0.95500976]
 #Deep+LR_+FM:   MEAN=0.9476 0.9400   [0.9490024, 0.94657576, 0.9401189, 0.94825876, 0.95636916, 0.95200694, 0.94006425, 0.94265074, 0.94319284, 0.9583263]
 -> #DeepFM超过原始FM，更超过embeddingLookUp的FM啦！

 FM2是等价的FM实现方式：每个field，和平方减平方和(None,k)，最后求和再*0.5
 Deep+LR_+FM2:   MEAN=0.9474 0.9413   [0.94403505, 0.944647, 0.95472664, 0.9430393, 0.9469636, 0.94908786, 0.95835567, 0.9435927, 0.94131535, 0.9480291]
(new computer CPU: LR1.1897~1.19 ;FM 1.02~1.03 GPU:LR1.187~1.19 FM:1.02)
-------------------

#movie lens 1M
    Feature: user_id+ movie_id
         LR: 1.1136   [1.11374,1.11363,1.11355]
   (K=16)FM: 0.8759 0.8725 [0.8725196239314502, 0.8778558374960211, 0.8761410918416856, 0.8767934833900838, 0.8764116537721851]
(2k.k.k)MLP: 0.8909 0.8874 [0.8905830239947838, 0.8876157716859745, 0.8874417751650266, 0.8929219301742843, 0.8958153680910038]
LRWide&Deep: 0.8894 0.8864 [0.8905002132246765, 0.8884505889083766, 0.8942822494084322, 0.8872838225545763, 0.886356520652771]
FMWide&Deep: 0.8876 0.8862 [0.8891306483292881, 0.8864404663254943, 0.8862015399751784, 0.8870562556423719, 0.8892361102224905]
     DeepFM: 0.8915 0.8882 [0.8891181369371052, 0.8885330240937728, 0.8965300496620467, 0.894879262809512, 0.8881970309003999]

--
同参数Feature: user_id+movie_id+gender+age+occ [6040, 3706, 2, 7, 21]
         LR:1.1138
         FM:0.8900 0.8874
        MLP:0.8869 0.8827 [0.8826913429211967, 0.8865704888029944, 0.8894205404233329, 0.8848255771624891, 0.8911240322680413]
FMWide&Deep:0.8826 0.8802 [0.8823739401901824, 0.8837272431277021, 0.8806559122061428, 0.8857936966268322, 0.880220939388758]
     DeepFM:0.8820 0.8801 [0.8826602427265312, 0.8804598394828507, 0.8817998533007465, 0.8800670077529135, 0.8852888055994541]

   ---->AFM:0.8766 0.8739 [0.8756965901278242, 0.8739465453956701, 0.8757859254185157, 0.878804661503321, 0.8788787903664987]  比普通FM好了15个千分点 比最好的DeepFM还高6个千
[new envirment test]
   (GPU)AFM:0.8780
(GPU B=2000)0.8857 0.8848 [0.8861,0.8837,0.8848,0.8876,0.8866] #比b=500差了10个千，是否尝试下BN?
        输出model.全1.
(new computer CPU AFM : [0.877 0.878 0.879 0.880 0.881]| GPU AFM:[0.877,0.876,0.879])
(new computer CPU FM : | GPU FM:MEAN=0.8895 BEST=0.8873 FM无论CPU/GPU就跟旧设备一样. [0.8929,0.8888,0.8895,0.8888,0.8874])
        NFM:0.8855 0.8833 [0.8867756211304967, 0.8886955860294873, 0.8853611725795119, 0.883353234544585, 0.8833889285220375] 比普通FM好了7个千分，但比AFM差8个千（新设备几乎没有影响）.
    DeepAFM:0.8813 0.8787 [0.881185443793671, 0.8795960338809822, 0.8787062863760357, 0.8843611107596868, 0.8827619403223448] 比AFM低了4个千.说明MLP和LR把AFM搞差了.

    #1.N-AFM? 2.attention vs matmul vs conv 3.dual attention / attention deep cross
    1.
    2.#dual attention 并行 同时对c和k做attention：0.892  先对c再对k:0.89234 0.8872  ; 对最后的k做：0.8972
    3. 全连接和attention的区别：全连接是每个维度固定权重的映射，而attention是每个维度
    【SimpleAFM】:based on matmul!
    (0.8799 0.8779 [0.8779658202883563, 0.879289800004114, 0.8794466397430324, 0.8788896631590928, 0.8842831839489032])比AFM略差一丝，基本一样但有一个模型偏差大.
    原理(None,c,k)  mat W':c,1 --> (None,k,1)  mat p:k,1 -->(None,1)

    【ConvAFM】:based on conv!超越AFM三个千分点! current Best.
     -> matmul style:
    0.8724 0.8712 [0.8727966151660002, 0.873124818862239, 0.8726405109031291, 0.8723721763755702, 0.8712409137170526]
     -> conv2d style: 确认一致.  inp=oup=1
    0.8725 0.8706 [0.8716835280007954, 0.8745225996910772, 0.8705774947057796, 0.8743310920799835, 0.8714246131196807]
     -> inp=1,oup=2.
    0.8756 0.8727 [0.875331884547125, 0.8797903753534148, 0.8757225093962271, 0.8727051543284066, 0.8745067042640493]
    -> inp=1,oup=3.
    0.8776 0.8752 [0.8757838303529764, 0.875153649758689, 0.8797498521925528, 0.8811651723294318, 0.8763137322437914]
    -> inp=1,oup=4
    0.8772 0.8744 [0.8773693012285836, 0.8743984021717989, 0.8810061142414432, 0.8776589965518516, 0.8754049851924558]
    【Conv NFM】:
    nn=(c,8,4,1) 四个输出：0.8832 0.8804 [0.886200704152071, 0.8825128842003738, 0.8803520619114743, 0.8827658917330489, 0.8841606036017212]
    nn=(c,4,1)   三个输出：0.8827 0.8807 [0.8829400008237814, 0.8808369579194467, 0.8830305031583279, 0.8859907278531715, 0.8807316274582585]
    nn=(c,4)     两个输出: 0.8788 0.8748 [0.8794441081300567, 0.8788113112691083, 0.8838653140430209, 0.8769311541243444, 0.8748137321653245]
    nn=(c,1)     两个输出: 0.8739 0.8707 [0.8748212328440026, 0.8721325057971326, 0.8706626587276217, 0.8745743220365501, 0.8773065289364586]


同特征 对FM减掉依赖
         FM:0.8843 0.8827 [0.8837845201733746, 0.8834048621262176, 0.8850899094267737, 0.8827350067186959, 0.8866759037669701]  比全交叉FM好5个千
FMWide&Deep:0.8821 0.8779 [0.8836640089373046, 0.8844558421569535, 0.8779307045514071, 0.8817171700393097, 0.8827627062797546]  比全交叉FWD好2个千
     DeepFM:0.8798 0.8792 [0.8800812885731082, 0.8791612175446523, 0.879225922838042, 0.8800252790692487, 0.8807229491728771]   比全交叉DFM好3个千
        AFM:0.8825 0.8805 [0.883282467081577, 0.8826788224751436, 0.8825482526911965, 0.880550700048857, 0.8835271959063373]    比全交叉AFM【低】了7个千.说明attention已经capture到了


--
同参数Feature: user_id+movie_id+(gender+age+occ)+(genre) [6040, 3706, 2, 7, 21] 不收敛..? LR~1.117@EPOCH=2/3/4  MLP~1.118@EPOCH=1
--
do_1:+ movie genre 爆炸
do_2:+ FM消除依赖 FM_DependencyEliminate
do_3:+ CIN
--------------------------------

# sklearn:
    Ridge 0.961;
    Lasso 1.122;
    ElasticNet:1.122

# lgb:
    1.00835 (best param: 'max_depth':30,'num_leaves': 200)


下面tf不准：
=====================================
sklearn:
    lr:系数爆炸
    ridge:0.961
    lasso/elasticNet:1.122
lgb:
    1.00835 (best param: 'max_depth':30,'num_leaves': 200 )
tf:
    lr(w l2_norm):1.007 (best param:batch size=500,adam 0.01)
    fm:0.95373  (best param:batch size=500,adam 0.01)



-------------------------
movielens latest. tag prediction rmse lr=0.0005 batch=1024 early_stopping_rounds=15
        LR: 0.9458
        FM:Best Score: 0.7367297162303766  at round  14   0.7423419489623597  at round  15
        MLP:Best Score: 0.7887067210476838  at round  11
        DFM:Best Score: 0.7972475506874345  at round  10
        NFM:Best Score: 0.7448198249251206  at round  3
        AFM:Best Score: 0.7693624980207803  at round 13
       CAFM:Best Score: 0.7270582996416783  at round  8
       CNFM:Best Score: 0.7277697468519705  at round  4

re_run: lr=0.0002,N_EPOCH=100,batch_size=1024,early_stopping_rounds=15
    FM   0.7418 0.7374 [0.7420544477476589, 0.7432679714129825, 0.7463349678378174, 0.7373948548274504, 0.7397591127371936]
    MLP  0.8172 0.8005 [0.8005491305705676, 0.8304985414874233, 0.806352101312661, 0.8252118271206723, 0.8233442148560076]
    DFM  0.7985 0.7797 [0.8071864117129742, 0.8064103320888851, 0.8047545949618021, 0.7796648124360149, 0.7946412784219035]
    NFM  0.7474 0.7359 [0.7358814849986793, 0.7629101249865616, 0.7600697449892451, 0.7414941661846564, 0.7366691137322728]
    AFM  0.7586 0.7448 [0.754342629364065, 0.7447536173074142, 0.7647130870671006, 0.7620551974877067, 0.7670084105391927]
    CAFM 0.7274 0.7141 [0.718197048327444, 0.7141293010232859, 0.7403262024340422, 0.7151694378620843, 0.7489307504262983]
    CNFM 0.7250 0.7240 [0.7251382016617319, 0.7243603084151542, 0.7269466616228747, 0.7240004010082032, 0.7245057860763423]

----------------------------
终于发现BUG！！解决LR中的没有keepdims的问题
movie lens AFM: rounded_rmse K=256,lr=0.0002,N_EPOCH=100,batch_size=1024,early_stopping_rounds=15
     LR：0.5925
     FM：0.4707 at round  100
    MLP: 0.5526@20 0.5543@18 0.5545@26 --> LR=0.0001+batch_size=1024:

    lr=0.0004,N_EPOCH=100,batch_size=1024,early_stopping_rounds=15:
    FM:0.47583942247498817  at round  99

    lr=0.001,N_EPOCH=100,batch_size=4096,early_stopping_rounds=15: #注意是Protocol的valid loss.一般低于test loss
              LR 0.5937 0.5935 [0.5934619987515819, 0.5935057623601988, 0.5942060873015214, 0.5936962694373709, 0.593615538225192]
              FM 0.4716 0.4616 [0.4615745090797523, 0.47751223197442794, 0.48188190272961845, 0.4654127593302519, 0.47144765939299155] (虽然test不高，但valid score很不错)
             MLP 0.5547 0.5498 [0.5521643645579941, 0.5497829604146859, 0.5619770722332953, 0.5559272731911425, 0.5535220250555853] + 0.5528 0.5478 [0.5540120193759691, 0.5540566900214229, 0.5477884859023658, 0.5535655559401627, 0.5545174046128125]
    (layer=1)NFM 0.5026 0.4883 [0.49492170541934416, 0.4983836790237002, 0.4882567305561861, 0.5114914719708186, 0.5198597840151992]
    (layer=2)NFM 0.4799 0.4741 [0.4808606069718846, 0.4754995542844974, 0.4740632420448006, 0.4890896348872503, 0.47978667740726133]
(l2继续尝试BN&DROP调优)
(afm_factor256)
            AFM 0.4641 0.4579 [0.45789503709436424, 0.45887810412995106, 0.4742138503255528, 0.46002649873254, 0.4695296085442708]
            CFM 0.4449 0.4386 [0.44946042875928, 0.458209527794555, 0.4391687542211156, 0.43859848788728134, 0.4389438245490222]
  CFM 10 TIMES: 0.4445 0.4386 [0.44946042875928, 0.458209527794555, 0.4391687542211156, 0.43859848788728134, 0.4389438245490222, 0.45537909361138035, 0.43946298414329715, 0.44139103748948294, 0.4438456694458008, 0.4409773219274019]


            AFM(factor=8加速训练)：
                AFM(features_sizes, k=256, attention_FM=8,dropout_keeprate=0.9,lambda_l2=0.0)
                0.4568 0.4547 [0.4572698326594009, 0.4573921705527199, 0.45465291076328773, 0.45600533839921154, 0.45853017602842766] @about 70
  keeprate=0.9:
  lambda_l2=0.01 0.4600 0.4533 [0.4533383172789684, 0.4556978576284572, 0.4559235026324993, 0.46789378198860504, 0.4673594533672846]
  lambda_l2=0.1 0.4545 0.4517 [0.4539906716335104, 0.45171106174205944, 0.45947772612652515, 0.45273780400588653]
  lambda_l2=0.5 0.4569 0.4505 [0.45365546628652675, 0.45050660377710666, 0.466321902291068, 0.45288184553374505, 0.46128432840955386] ==> AFM BEST
  lambda_l2=1.0 0.4628 0.4558 [0.46781114589352274, 0.46781114589352274, 0.45993904670218094, 0.455809302852129]

                AFM(features_sizes, k=256, attention_FM=8,dropout_keeprate=0.9,lambda_l2=0.5)
                0.4600 0.4533 [0.4533383172789684, 0.4556978576284572, 0.4559235026324993, 0.46789378198860504, 0.4673594533672846]

                keetrate=0.8: drop@FM out:lr=0.001 0.455*2 0.458 0.459 @about 90 增大学习率
                keetrate=0.8: drop@FM out:lr=0.002 0.458*2 0.461
                keetrate=0.5: drop@FM out:lr=0.002 0.470 0.475~0.576@50

            CFM drop=0.9 lambda_l2=0.0
            0.4399 0.4377 [0.4401421597430921, 0.4445318712155105, 0.4386408222350688, 0.43773401371670007, 0.4382561142294935] ==> CFM BEST
            CFM drop=0.8 lambda_l2=0.0
            0.4428 0.4416 [0.4453344480139384, 0.44203669836782483, 0.4432974347651861, 0.4416111090133734, 0.4416171192475049]
            CFM drop=0.9 lambda_l2=0.001
            0.4460 0.4445 [0.4445430558011853, 0.4463642851494561, 0.44487508261756536, 0.446464587102342, 0.44768517511340944]
            CFM drop=0.9 lambda_l2=0.01
            0.4599 0.4575 [0.45806877558385833, 0.45753119090607136, 0.46346873364072205, 0.4595067564539701, 0.46098559930914673]
            CFM drop=0.9 lambda_l2=0.1
            炸了 [0.4894827117259846,0.5290414669703993,0.546778893353624,0.5028531207900812]

            drop错了不能drop embed：
            AFM(factor=8)  +drop0.5: 0.49338105986929937  at round  90 (似乎还可以继续收敛)
            AFM(factor=256)+drop0.5: 0.4966860595486163   at round  89
考虑dropout要加大学习率     +drop0.5: lr*10=0.01 0.5225 at 25 错

1.drop for conv2d
2.+reg +drop
3.引用AFM的论文


avazu
LR: 0.3902 @ epoch 2 #0.0005->0.001(1e-3 bs=1000)

['device_id', 'site_id']: Best Score: 0.41213182015337585  @ 15  bs=6000 lr=0.001



[last.fm 360K]
metrics: RMSLE(RMSE of log target) 随机推荐：RMSLE=1.973

features=['userId', 'artistId']
LR:0.6736@e24    hash 250W 1.1619@e2
FM:0.71115@3(k=128 lr=0.001) 0.7049@6(k=128 lr=0.0005) 0.701725@4(k=256,lr=0.0005) 0.6853@7(k=42,lr=0.0005) 0.68864@50(k=24,lr=0.0005)
说明k不是越大越好.太长的隐向量反而不容易学习 短一些稍微影响精度但是模型会更优

features=['userId', 'artistId', 'gender', 'age', 'country']
LR:     0.6886@50(lr=0.0005,b=5000)收敛
FM:     0.6773@17(k=24,lr=0.0005,b=5000)
MLP:    0.7009 0.6999 [0.7013, 0.6999, 0.7016] (24,24) k=24 b=5000
        MLP:0.7070@4 (24,24) k=24 b=1024 bs无影响
        MLP:0.7062@4 (96,48) k=24 b=5000 width增大无提升 反而容易过拟合
        MLP:0.6845@5 (12,12) k=24 b=5000 width减小效果更好 也更容易训练
        MLP:0.6999@9 (8,8)   k=24 b=5000
FMDE:   0.6886    (k=24,lr=0.0005,b=5000,FM_ignore_interaction=[(0,2),(0,3),(0,4)])
        0.6878@25 (k=24,lr=0.0005,b=5000,FM_ignore_interaction=[(0,1),(0,2),(0,3),(0,4)])
WND:    0.6968@6 ((12,12),k=24,lr=0.0005,b=5000) 0.6997@5
FMNDeep:0.6999@5
DeepFM: 0.6999@5 ((12,12),k=24,lr=0.0005,b=5000)
    deepNet have strong overfitting:
    DeepFM:0.70574@5 ((8,8),k=24,lr=0.0005,b=5000)
NFM:    0.6886@50  default fmout-k-1 [train_loss:0.6707]
        0.6790@9   fmout-1  [train_loss:0.6604]


[KKBOX's music recommendation]--simulate work data
<Valid Score>
(old ver. using logloss metrics)
train_features=['msno', 'song_id']
                logloss        AUC      epoch       trainloss
LR              0.6372        0.6972     13          0.5992
FM(k=8)         0.5949        0.6061     22          0.5269


<sklearn real AUC> KKBOX's benchmark
BUG1: 传.fit()传了完整df而不是copy，导致内部改变了外部数据,test时出错
BUG2：ColdStartEncoder 多个列的时候发现test_AUC~0.5  看数据发现全是0
(new ver. use sklearn auc instead of tf.metrics.auc for precision) [BUG 1&2 FIXED]
train_features=['msno', 'song_id'] lr=0.0005, N_EPOCH=50, batch_size=4096,early_stopping_rounds=5
             valid_AUC    epoch    test_AUC    train_AUC    Kaggle    params
LR              0.7347     27       0.6876       0.7610     0.6229    valid 0.7339~0.7349
FM(k=8)         0.7569     24       0.6777       0.8303     0.6110    test 0.6835 0.5712 0.5576           [bug前 0.7569+0.5823+K0.5711]
MLP(k=8;(16,8)) 0.7330     4                     0.7684
MLP(k=8;(16,8)) 0.7341     4                     0.7547               lr=0.0001
MLP(k=8;(8,8))  0.7342     9        0.6774       0.7916     0.6118    lr=0.0001  改bs=1024是0.7347 same   [bug前 0.7348+0.5789+K0.5676]
NFM(k=8,k->1)   0.7472     13                    0.8237               不如FM.
WND(k=8,(8,8))  0.7334     2        0.6852       0.7498     0.6212    0.737~0.739 [RUN1 0.7341@6+0.6758+K?]    [bug前 0.7393+0.5819+K?]
DFM(k=8,(8,8))  0.7397     4        0.5828       0.7693               0.7366+0.5828@2

交互信息+用户侧信息 F5
train_features=['msno','song_id','city','bd','gender'] lr=0.0005, N_EPOCH=50, batch_size=4096,early_stopping_rounds=5
             valid_AUC    epoch    test_AUC    train_AUC    Kaggle    params
LR              0.7351     25       0.6873       0.7603     0.6232    run2:0.7347+0.6880+train0.7610
FM(k=8)         0.7344     3        0.6867       0.7551     0.6232
FMDE(k=8,0&234) 0.7295     24       0.6770       0.8291     0.6094
MLP(k=8;(8,8))  0.7332     6        0.6832       0.7684               [bs=512:valid0.7400+test0.6850+train0.7765@5]
NFM(k=8,k->1)   0.7343     2        0.6868       0.7500
NFM(k=8,k-8-1)  0.7338     2        0.6858       -
WND(k=8,(8,8))  0.7383     3        0.6768       0.7647
DFM(k=8,(8,8))  0.7340     1        0.6875       0.7011     0.6219     run2:0.7340+0.6875+train0.7011
AFM(k=8,t=8)    0.7351     2        0.6853       0.7484     0.6222
CFM(k=8,t=8)    0.7316     2        0.6852       0.7437                impl2CFM error:Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
CFM(sameimpl2)  0.7334     2        0.6860       0.7474                solved by updating drivers.
a=pd.DataFrame([[0.6876,0.6229],[0.6777,0.6110],[0.6774,0.6118],[0.6852,0.6212],[0.6873,0.6232],[0.6867,0.6232],[0.6770,0.6094]])
a.corr()#0.993582!
a.plot()

https://github.com/lystdo/Codes-for-WSDM-CUP-Music-Rec-1st-place-solution/blob/master/nn_structure.pdf

交互+用户侧+歌曲侧  F8
train_features=['msno','song_id','city','bd','gender','genre_ids','artist_name','language'] lr=0.0005, N_EPOCH=50, batch_size=4096,early_stopping_rounds=5
             valid_AUC    epoch    test_AUC    train_AUC    Kaggle    params
LR              0.7374     25      0.6980       0.7628      0.6292
FM(k=8)         0.7507     5       0.7047       0.7851      0.6267    [valid0.7368+test0.6955+train0.7538@3; valid0.7470+test0.7023+train0.8068@12;  valid0.7445+test0.6986+train0.8320@35; valid0.7365+test0.6970+train0.7549@3;]
MLP(k=8;(8,8))  0.7433     9       0.6968       0.8021
MLP(k=8;(32,32  0.7394     7       0.6930       0.8072                [valid0.7408+test0.6912+train0.8192@8]
NFM(k=8,k->1)   0.7373     2       0.6956       0.7528
NFM(k=8,k-8-1)  0.7544     5       0.7074       0.7945      0.6231    NFM-K81_F8_valid0.7544_test0.7074.csv
WND(k=8,(8,8))  0.7359     2       0.6937       0.7418
DFM(k=8,(8,8))  0.7460     7       0.7001       0.7966      0.6197     valid0.7468+test0.6986+train0.8118@14+kaggle0.61707
AFM(k=8,t=8)    0.7367     3       0.6970       0.7580      0.6265
CFM(k=8,t=8)    0.7504     17      0.7010       0.8461      0.6103     valid0.7521_test0.7038_train0.8347@15+kaggle0.61416
DAFM(kt=8,(8,8) 0.7500     10      0.6934       0.8105      0.6167

交互+用户侧+歌曲侧+上下文 F11
train_features=['msno','song_id','city','bd','gender','genre_ids','artist_name','language','source_system_tab','source_screen_name','source_type']
             valid_AUC    epoch    test_AUC    train_AUC    Kaggle    params
LR              0.7652     24      0.7345       0.7858      0.6594
FM(k=8)         0.7910     23      0.7511       0.8598      0.6492
MLP(k=8;(8,8))  0.7770     5       0.7414       0.8082      0.6586
MLP(k=8;(32,8   0.7849     7       0.7436       0.8430      0.6526
NFM(k=8,k->1)   x
NFM(k=8,k-8-1)  0.7911     9       0.7486       0.8516      0.6509    0.7908    12      0.7493       0.8614
WND(k=8,(8,8))
DFM(k=8,(8,8))
AFM(k=8,t=8)
CFM(k=8,t=8)
DAFM(kt=8,(8,8)

COLDAVG:
             valid_AUC    epoch    test_AUC    train_AUC    Kaggle    params
LR              0.7651     25      0.7334       0.7862      0.6573
FM(k=8)         0.7932     20      0.7470       0.8580      0.6439


TIME:
             valid_AUC    epoch    test_AUC    train_AUC    Kaggle    params
LR              0.6779     22      0.6448       0.7960      0.6485
LR COLD         0.6772     21      0.6425       0.7951      0.6468
FM(k=8)         0.6775     3       0.6424       0.7862      0.6465
FM(k=8)COLD     0.6768     3       0.6424       0.7866      0.6455
MLP(k8(8,8))    0.6797     2       0.6419       0.7867      0.6434
MLP(k8(8,8))COLD0.6794     2       0.6423       0.7855      0.6445
DFM             0.6817     5       0.6452       0.8236      0.6421
AFM             0.6780     2       0.6417       0.7769      0.6451
+notebook:
import seaborn as sns
sns.regplot(x='song_length',y='target',data=train[['song_length','target']].sample(300000))

+
sess=tf.InteractiveSession()
tt=tf.Variable(np.array([[1,2,3],[4,5,6],[7,8,9]],dtype='float32'))
init=tf.global_variables_initializer()
sess.run(init)
print(tt.eval())#type:refVariable

#tt=tt[0,:].assign(tf.zeros(3))
#tt.eval()


mean=tf.reduce_mean(tt[1:3,:],axis=0,keepdims=False)
print(mean.eval())  #array([5.5,6.5,7.5])

op=tt[0,:].assign(mean)
sess.run(op)
print(tt.eval())

features_sizes
Out[48]: [28620, 314736, 22, 93, 4, 547, 37337, 11, 10, 22, 14]
[0,0+28260,28260+314736]
Out[49]: [0, 28260, 342996]

#in runing
tt=model.model.w.eval(session=model.model.sess)
print(tt[0,:])
print(tt[1:28260,:].mean(axis=0))
print(tt[28620,:])
print(tt[28621:28260+314736,:].mean(axis=0))

tt=model.model.embedding_weights.eval(session=model.model.sess)
print(tt[0,:])
print(tt[1:28260,:].mean(axis=0))

print(tt[28620,:])
print(tt[28621:28260+314736,:].mean(axis=0))
